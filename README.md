# IAML Intelligent Autonomous Machine Learning<br />
"I Am Machine Learning"<br />
project codename = codephreak<br />
"Professor Codephreak is an expert in machine learning, computer science and computer programming ..."<br />
uiux.py provides Gradio output to local server for local language model interaction<br />
Gradio interacts with html javascript and css <a href="https://www.gradio.app/guides/custom-CSS-and-JS">custom Gradio</a><br />
<a href="https://www.gradio.app/guides/getting-started-with-the-js-client">Gradio Javascript Client</a><br />
Documentation: codephreak = uiux.py + memory.py + automind.py + aglm.py

In this documentation, we will explore how the Python files uiux.py, memory.py, automind.py, and aglm.py interact with each other to create a conversational UI interface and generate contextual responses. Provided is a  detailed overview of the interactions and the outcome of running these files together. modular design has been intionally chosen to serve as contextual agent interaction and addition of the executable agent module automindx.py

    uiux.py:
This file contains code for creating a GUI-based conversational user interface using the gradio library. Users can interact with various language models through this UI. The main components include:

    run_ui(model, tokenizer, is_chat_model, model_type, save_history=True):
This function sets up the UI interface using gradio. It takes a pre-trained model, a tokenizer, flags to indicate model type and whether it's a chat model, and a history-saving option. Users can input messages in a chat-like interface, and the responses are generated and displayed.

        User Interaction Flow:
            The user inputs a message through the UI.
            The run_ui function captures the user input and prepares it for processing.
            The user input is passed to the model for generating a response.
            The response is displayed on the UI.

        Outcome:
            Users can have interactive conversations with language models through the UI, receiving responses in real-time.
            Conversations are displayed in a chat format, with both user inputs and model responses visible.

    memory.py:
This file handles the management and storage of conversation memory. It provides functions to save conversation history and format it for later use. Key components include:

        save_conversation_memory(memory):
This function takes a conversation memory as input, formats it, and saves it as a JSON file. Each memory entry consists of user instructions and model responses.

        Outcome:
            Conversation history is saved in JSON files, allowing users to refer back to previous interactions.
            The formatted memory can later be utilized for analysis, training, or generating contextual responses.

    automind.py:
This file provides functions to format conversation data into "llama chat style," which is suitable for input to the "Llama" model. It prepares conversation context for generating contextual responses. Key components include:

        format_to_llama_chat_style(memory) -> str: 
This function formats conversation memory into a specific style called "llama chat style." It appends system prompts and instruction-response pairs.

        Outcome:
            Conversation memory is transformed into a format that can be readily used by the "Llama" model for generating responses.
            The formatted context is designed to provide meaningful and coherent prompts to the model.

    aglm.py: 
This file interacts with conversation memory data and the "Llama" model to generate contextual responses. It loads memory, prepares conversation context, and generates responses. Key components include:

        LlamaModel class: Initializes and manages the "Llama" model and tokenizer.

        generate_contextual_output(conversation_context): 
This method generates a response based on conversation context by formatting it, encoding it, and decoding the model's output.

        main(): 
Orchestrates the process by loading memory data, batching conversations, initializing the model, and generating responses.

        Outcome:
            Contextual responses are generated based on the entire conversation history using the "Llama" model.
            The responses are coherent and informed by the conversation's context.

    Interactions and Flow:
        Users interact with the UI created by uiux.py, having conversations with language models.
        User inputs and model responses are collected and stored in conversation memory through memory.py.
        Conversation memory is formatted into "llama chat style" using functions from automind.py.
        The formatted conversation context is used by aglm.py to generate contextual responses through the "Llama" model.

    Overall Outcome:
        Users can engage in interactive conversations with language models through the UI.
        Conversations are recorded and formatted for future use.
        The "Llama" model generates meaningful responses based on the context of the entire conversation history.

This integrated system allows users to have dynamic conversations, store and retrieve interaction history, and receive informed responses based on the context of the ongoing conversation. The outcome is a sophisticated conversational interface that leverages various Python files to provide an engaging and contextually relevant experience.

to do<br />
add download model to UI<br />
add wgetter<br />
add gitgetter<br />
add automindx (executable folder sandbox)<br />
add deployai gpt-auto function calling<br />
add multimodel parsing from aglm.py

